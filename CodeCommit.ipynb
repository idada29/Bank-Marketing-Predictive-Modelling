{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section one: Packages installing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "import scipy\n",
    "\n",
    "#This is to label all categorical data\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "#This is plotly module used for plotting\n",
    "import plotly.express as px\n",
    "\n",
    "import IPython\n",
    "import sklearn\n",
    "\n",
    "# Importing the required packages for Decision Tree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#This packages must be installed for some visualizations to work properly\n",
    "#!pip install cufflinks\n",
    "#!pip install chart_studio\n",
    "#!pip install plotly\n",
    "\n",
    "#This is needed to make use of pandas profilling to show the break down of the data\n",
    "#conda install -c conda-forge pandas-profiling\n",
    "\n",
    "# This is need to print out notebook as pdf\n",
    "#pip install -U notebook-as-pdf\n",
    "#conda install nbconvert\n",
    "\n",
    "# Display the Dive visualization for the training data.\n",
    "from IPython.core.display import display, HTML\n",
    "#!pip install facets-overview\n",
    "\n",
    "# SECTION 2 EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "# The two csv found on the link were combined to have an increased observation\n",
    "banking_data = pd.read_csv(\"C:\\\\Users\\\\dadai\\\\Downloads\\\\Machine Learning\\\\ICA\\\\BANKING DATA FROM UCI\\\\BANK MARKETING\\\\bank-combined.csv\")\n",
    "\n",
    "banking_data.shape\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "banking_data.profile_report() \n",
    "\n",
    "# This piece of code was used to revalidate that this items here represented \n",
    "# as duplicated above doesn't mean they are actually duplicated rows entirely \n",
    "#but we have some rows whose values tallies with some values from other \n",
    "#rows for more than 5-7 columns\n",
    "\n",
    "duplicateRowsDF = banking_data[banking_data.duplicated()]\n",
    "duplicateRowsDF.head(10)\n",
    "\n",
    "banking_data.describe(include= \"all\")\n",
    "\n",
    "banking_data.info()\n",
    "\n",
    "# Check for missing values in the training dataset, \n",
    "# output indicates that our dataset has no null values\n",
    "\n",
    "print(banking_data.isnull().values.any())\n",
    "print(\" \")\n",
    "print(banking_data.isnull().sum())\n",
    "\n",
    "# EXPLORATORY DATA ANALYSIS (EDA) AND DATA PREPROCESSING\n",
    "\n",
    "distinct_marital = banking_data.marital.value_counts()\n",
    "print('The distinct value of marital status \\n{}'.format(distinct_marital))\n",
    "print('')\n",
    "\n",
    "distinct_education = banking_data.education.value_counts()\n",
    "print('Diiferent education types within the data: \\n{}'.format(distinct_education))\n",
    "print('')\n",
    "\n",
    "distinct_default = banking_data.default.value_counts()\n",
    "print('Any previous defaulted obligations: \\n{}'.format(distinct_default))\n",
    "print('')\n",
    "\n",
    "distinct_housing = banking_data.housing.value_counts()\n",
    "print('Owners of houses: \\n{}'.format(distinct_housing))\n",
    "print('')\n",
    "\n",
    "distinct_loans = banking_data.loan.value_counts()\n",
    "print('Have outstanding loans: \\n{}'.format(distinct_housing))\n",
    "print('')\n",
    "\n",
    "distinct_contact = banking_data.contact.value_counts()\n",
    "print('The mode of contact and frequency: \\n{}'.format(distinct_contact))\n",
    "print('')\n",
    "\n",
    "distinct_outcome = banking_data.poutcome.value_counts()\n",
    "print('Outcome for previous marketing campaigns: \\n{}'.format(distinct_outcome))\n",
    "print('')\n",
    "\n",
    "\n",
    "# SECTION 3 PREPROCESSING AND HANDLING CLASS IMBALANCE\n",
    "\n",
    "# I noticed an imbalanced distribution of class of our target feature. I will be attending to this few steps below \n",
    "target_ratio = banking_data['y'].value_counts()\n",
    "print(\"The classes of our binary target: \\n{}\".format(target_ratio))\n",
    "\n",
    "\n",
    "# The dataset target is really skewed and class of target is imbalance as shown below, \n",
    "# however using KNN to attend to this by selecting samples within clusters formed\n",
    "# in the majority class\n",
    "\n",
    "fig = px.pie(banking_data['y'].value_counts().reset_index(), values = 'y',\n",
    "             names = ['no', 'yes'])\n",
    "fig.update_traces(textposition = 'inside', textinfo = 'percent + label', hole = 0.6, \n",
    "                  marker = dict(colors = ['#0e4bef','#ffcccb'],\n",
    "                                line = dict(color = 'white', width = 3)))\n",
    "# Add text labelling\n",
    "fig.update_layout(annotations = [dict(text = 'Target', \n",
    "        x = 0.5, y = 0.5,font_size = 24, showarrow = False, \n",
    "        font_family = 'Times New Roman',font_color = 'black')],showlegend = False)           \n",
    "fig.show()\n",
    "\n",
    "# calculate correlation matrix\n",
    "grid_kws = {'height_ratios':(0.9,0.05),'hspace':0.3}\n",
    "f,(ax,cbar_ax)=plt.subplots(2,gridspec_kw=grid_kws)\n",
    "\n",
    "corr = banking_data.corr()\n",
    "\n",
    "ax= sns.heatmap(corr,annot=True,annot_kws={'fontsize':12},\n",
    "                linewidths= 1,ax=ax,cbar_ax=cbar_ax,\n",
    "                cbar_kws={'orientation':'horizontal'})\n",
    "\n",
    "corr = banking_data.corr()\n",
    "mask= np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style('white'):\n",
    "    f,ax= plt.subplots(figsize=(12,5))\n",
    "    ax= sns.heatmap(corr,mask=mask,annot=True,annot_kws={'fontsize':10},linewidths= 1,vmax=7,square = True)\n",
    "\n",
    "# USING CLUSTER & SAMPLING TO ADDRESS IMBALANCED CLASS WITHIN THE DATASET\n",
    "\n",
    "feature_names = banking_data.columns\n",
    "\n",
    "# In order to further understand our dataset, identify the columns with numeric data against categorical to enable proper grouping\n",
    "# and exploration.\n",
    "\n",
    "numericColumns= banking_data._get_numeric_data().columns\n",
    "nCols = list(numericColumns[0:])\n",
    "categoricalColumns = list(set(feature_names) - set(numericColumns))\n",
    "categoricalColumns.remove('y')\n",
    "\n",
    "print ('The numericColumns are: {}'.format(nCols))\n",
    "print('\\n')\n",
    "print('The categoricalColumns are: {}'.format(categoricalColumns))\n",
    "\n",
    "# We need to convert the categorical data into indicator variables (0 or 1) and also scale the numerical features\n",
    "categorical_features = pd.get_dummies(banking_data[categoricalColumns])\n",
    "\n",
    "\n",
    "# What the above those is that: it selects on the cateogrical columns, then creates a separate column for each of the \n",
    "# elements/values within that column assisgning a value of 1 to it if present and zeros (0) to others. Thus increasing the number \n",
    "# of columns but having all the category now represented as 0 or 1\n",
    "\n",
    "categorical_features.head()\n",
    "\n",
    "# Next is to scale numeric features, for this I made use of standardScaler()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "numeric_features = banking_data[numericColumns]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#The scaled features is then converted back to dataFrame to enable me merge/concatenate it with the categorical features\n",
    "scaled_numeric_features = pd.DataFrame(scaler.fit_transform(numeric_features), columns=numeric_features.columns)\n",
    "\n",
    "# Concatenate the the categorical_feature and scaled numeric features\n",
    "new_data = pd.concat([categorical_features, scaled_numeric_features], axis=1)\n",
    "new_data.shape\n",
    "\n",
    "# You will observe at this point that the number of columns increased, hence a need for PCA to determine component selection / subset\n",
    "# which best explains the data\n",
    "\n",
    "new_data.head()\n",
    "\n",
    "# Feature Extraction with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca_transformed_data = pca.fit_transform(new_data)\n",
    "\n",
    "df = pd.DataFrame(pca_transformed_data)\n",
    "headers = ['PC1','PC2','PC3','PC4']\n",
    "df.columns = headers\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "#Extract all values from the 3 principal components\n",
    "PCA_values = df[['PC1','PC2','PC3']].values\n",
    "\n",
    "# Import KElbowVisualizer and KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use yellowbrick cluster to determine the number of cluster based on nearest neighbours\n",
    "visualizer = KElbowVisualizer(KMeans(),k=(2,11))\n",
    "visualizer.fit(PCA_values)\n",
    "visualizer.show(outpath='elbowplot.png')\n",
    "\n",
    "n_clusters = 4 #From the elbow plot\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "#To find out the centroid which would enable us fit properly with KMeans\n",
    "centroid, label = kmeans2(pca_transformed_data,n_clusters, minit='points')\n",
    "centroid\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters, max_iter=10000, verbose=1, n_jobs=4, init=centroid)\n",
    "\n",
    "clusters = kmeans.fit_predict(df)\n",
    "\n",
    "#Add a new columns for the predicted KMeans cluster prediction\n",
    "df['cluster'] = clusters\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Plot to visualize display the relationships of the principal components \n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5, 5))\n",
    "plt.scatter(df[['PC2']], df[['PC3']], c=df['cluster'])\n",
    "ax.set_xlabel(\"PC2\",fontsize=12)\n",
    "ax.set_ylabel(\"PC3\",fontsize =12)\n",
    "ax.set_title(\"Comparing Principal components\",fontsize = 14)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5, 5))\n",
    "plt.scatter(df['PC1'], df['PC2'], c=df['cluster'])\n",
    "ax.set_xlabel(\"PC1\",fontsize=12)\n",
    "ax.set_ylabel(\"PC3\",fontsize =12)\n",
    "ax.set_title(\"Comparing Principal components PC1 and PC2\",fontsize = 14)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5, 5))\n",
    "plt.scatter(df['PC1'], df['PC2'], c=df['cluster'])\n",
    "ax.set_xlabel(\"PC1\",fontsize=12)\n",
    "ax.set_ylabel(\"PC2\",fontsize =12)\n",
    "ax.set_title(\"Comparing Principal components PC1 and PC2\",fontsize = 14)\n",
    "\n",
    "# Convert target no and yes into 0 and 1 and store in a variable to be used later\n",
    "target_labels = banking_data['y'].map({'no':0, 'yes':1})\n",
    "\n",
    "# Select a sample from each quarter of the minor class\n",
    "minority_class = target_labels.value_counts()[1]\n",
    "\n",
    "# Divide the minority class and discard the remainder or decimal \n",
    "sample_size = minority_class//n_clusters\n",
    "\n",
    "print(\"The sample size to be considered when undersampling the majority class is: {}\".format(sample_size))\n",
    "\n",
    "\n",
    "#We need to select random data points samples of the majority class 0 or no\n",
    "my_list = []\n",
    "\n",
    "for value in range(0,n_clusters):\n",
    "    # create an array of index for which selects majority target_labels = 0 and concides \n",
    "    # with iteration of value of the different clusters\n",
    "    majority_class = df[(target_labels==0) & (df['cluster'] == value)].index\n",
    "    \n",
    "    # Using np.random.choice function, to select a random size/samples which is the \n",
    "    # same as the size of samples selected from the minority_samples \n",
    "    my_list.append(np.random.choice(majority_class, sample_size))   \n",
    "        \n",
    "# Finally, using np.ravel, the shape list stored in my_list is reshaped into a \n",
    "# flattened aray without any specific order      \n",
    "my_list = np.ravel(my_list)\n",
    "\n",
    "# The items in the array within this represents only 1452 samples of each clusters of the majority class 'no' \n",
    "print(np.count_nonzero(my_list))\n",
    "\n",
    "# The resampled data is the concatenation of all the minority class and the unsampled majority class (no)\n",
    "resampled_data = pd.concat([banking_data.iloc[my_list], banking_data[banking_data['y'] == 'yes']])\n",
    "print(resampled_data['y'].value_counts())\n",
    "\n",
    "# A FEW THINGS TO NOTE:\n",
    "\n",
    "# 1. Resampled_data would always be a new subset of the previous clusters because we induced np.random in randomly selecting the different values of variable choices and new samples above\n",
    "# 2. In order to have the record of the resampled_data you initated at the point, we would be saving the resampled_data to your local path to enable us reaccess whenever we need it.\n",
    "\n",
    "\n",
    "resampled_data.to_csv(\"Resampled_data.csv\")\n",
    "\n",
    "resampled_data.head()\n",
    "\n",
    "# The final results of target class distribution is shown below\n",
    "\n",
    "fig = px.pie(resampled_data['y'].value_counts().reset_index(), values = 'y',\n",
    "             names = ['no', 'yes'])\n",
    "fig.update_traces(textposition = 'inside', \n",
    "                  textinfo = 'percent + label', \n",
    "                  hole = 0.6, \n",
    "                  marker = dict(colors = ['#0e4bef','#ffcccb'],\n",
    "                                line = dict(color = 'white', width = 3)))\n",
    "\n",
    "# Add text labelling\n",
    "fig.update_layout(annotations = [dict(text = 'Target', \n",
    "                                      x = 0.5, y = 0.5,\n",
    "                                      font_size = 24, showarrow = False, \n",
    "                                      font_family = 'Times New Roman',\n",
    "                                      font_color = 'black')],\n",
    "                  showlegend = False)\n",
    "                  \n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# SECTION 4 DATA VISUALIZATIONS \n",
    "\n",
    "# A plot to show the relationship of different features as compared to the target\n",
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(resampled_data, hue=\"y\", palette=\"Set1\")\n",
    "\n",
    "# Visualizing relationship between categorical columns and balance\n",
    "fig = px.box(resampled_data,x = \"job\",color=\"y\",orientation='v',title=\"Distribution of target outcome by different job types\")\n",
    "fig.show()\n",
    "\n",
    "# We have to also show Graph to show the frequency of target outcome by different job types below:\n",
    "graphs = pd.concat([resampled_data['job'][resampled_data.y=='yes'].value_counts(),resampled_data['job'][resampled_data.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs.plot(kind='bar',color=['red','blue'],title= 'Target outcome responses by different job types',ylabel= 'Count',figsize = (15, 5))\n",
    "\n",
    "\n",
    "# From the output we see that more single and divorced tends to want and buy in to a marketed products\n",
    "\n",
    "\n",
    "#Comparing account balances for customers with house loans or not as against our target outcome\n",
    "fig = px.box(resampled_data,x = \"housing\",y=\"balance\",color=\"y\",points = \"all\",orientation='v',title=\"Comparing account balances by customers with house loans or not as against our target outcome\")\n",
    "fig.show()\n",
    "\n",
    "# We have to also show Graph to show the frequency of target outcome by different housing :\n",
    "graphs = pd.concat([resampled_data['housing'][resampled_data.y=='yes'].value_counts(),resampled_data['housing'][resampled_data.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs.plot(kind='bar',color=['red','blue'],title= 'Target outcome responses by if customers currently has a house loan or not',ylabel= 'Count',figsize = (10, 5))\n",
    "\n",
    "\n",
    "\n",
    "#Distribution of personal loans and balance compared to the target outcome\n",
    "fig = px.box(resampled_data,x = \"loan\",y=\"balance\",color=\"y\",points = \"all\",orientation='v',title=\"Distribution of personal loans and balance with respect to the target outcome\")\n",
    "fig.show()\n",
    "\n",
    "# We have to also show Graph to show the frequency of target outcome by different customers having a pending loan or not:\n",
    "graphs = pd.concat([resampled_data['loan'][resampled_data.y=='yes'].value_counts(),resampled_data['loan'][resampled_data.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs.plot(kind='bar',color=['red','blue'],title= 'Target outcome responses by if customers is a beneficiary of a personal loan with the bank or not',ylabel= 'Count',figsize=(10,5))\n",
    "\n",
    "\n",
    "\n",
    "#Comparing account balances by marital status as against our target outcome\n",
    "fig = px.box(resampled_data,x = \"marital\", y = \"balance\",color=\"y\",points=\"all\",title=\"Comparing account balances by marital status as against our target outcome\")\n",
    "fig.show()\n",
    "\n",
    "# We have to also show Graph to show the frequency of target outcome by different marital status:\n",
    "graphs = pd.concat([resampled_data['marital'][resampled_data.y=='yes'].value_counts(),resampled_data['marital'][resampled_data.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs.plot(kind='bar',color=['red','blue'],title= 'Target outcome count by different marital status',ylabel= 'Count',figsize=(10,5))\n",
    "\n",
    "\n",
    "# From the output we see that more single and divorced tends to want and buy in to a marketed products\n",
    "\n",
    "# Comparing account balances by education type\n",
    "fig= px.box(resampled_data,x = \"education\", y = \"balance\",color='y',title=\"Comparing account balances by education type\",notched=True,points = \"all\")\n",
    "fig.show()\n",
    "\n",
    "# The graph above shows the distribution of responses by different education institution, however the bar chart below drills down\n",
    "# to comparing by count of the target outcome.\n",
    "\n",
    "graphs = pd.concat([resampled_data['education'][resampled_data.y=='yes'].value_counts(),resampled_data['education'][resampled_data.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs.plot(kind='bar',color=['red','blue'],title= 'Target outcome count by different educational institution',ylabel= 'Count',figsize=(10,5))\n",
    "\n",
    "# Comparing account balances by contact type\n",
    "\n",
    "fig = px.box(resampled_data,x = \"contact\", y = \"balance\",color='y',points= \"all\",notched=True,orientation='v',title=\"Comparing account balances by contact type\")\n",
    "fig.show()\n",
    "\n",
    "# We have to also show Graph to show the frequency of target outcome by different contact type below:\n",
    "graphs = pd.concat([resampled_data['contact'][resampled_data.y=='yes'].value_counts(),resampled_data['contact'][resampled_data.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs.plot(kind='bar',color=['red','blue'],title= 'Target outcome count by different contact type',ylabel= 'Count',figsize=(10,5))\n",
    "\n",
    "\n",
    "import plotly.offline as py\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#This plot compares balance and previous contact compared with the target outcomes\n",
    "resampled_data_pr_1 = resampled_data[resampled_data.y == 'yes']\n",
    "resampled_data_pr_2 = resampled_data[resampled_data.y == 'no']\n",
    "\n",
    "# For target outcome 'yes'\n",
    "trace1 =go.Scatter(\n",
    "                    y = resampled_data_pr_1.balance,\n",
    "                    x = resampled_data_pr_1.previous,\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Target: yes\",\n",
    "                    marker = dict(color = 'rgba(240, 136, 200, 0.8)'),\n",
    "                    text= resampled_data_pr_1.y)\n",
    "# For target outcome 'no'\n",
    "trace2 =go.Scatter(\n",
    "                    y = resampled_data_pr_2.balance,\n",
    "                    x = resampled_data_pr_2.previous,\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Target: no\",\n",
    "                    marker = dict(color = 'rgba(0, 130, 200, 0.8)'),\n",
    "                    text= resampled_data_pr_2.y)\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "# Plotting the layout\n",
    "layout = dict(title = 'Balance - previous contact - target',\n",
    "              xaxis= dict(title= 'Previous contact',\n",
    "                          ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'Balance',\n",
    "                          ticklen= 5,zeroline= False),\n",
    "             autosize=False,\n",
    "             width=700,\n",
    "             height=450,)\n",
    "fig = dict(data = data, layout = layout)\n",
    "    \n",
    "iplot(fig)\n",
    "\n",
    "#This plot compares balance and age compared with the target outcomes\n",
    "resampled_data_pr_1 = resampled_data[resampled_data.y == 'yes']\n",
    "resampled_data_pr_2 = resampled_data[resampled_data.y == 'no']\n",
    "\n",
    "# For target outcome 'yes'\n",
    "trace1 =go.Scatter(\n",
    "                    y = resampled_data_pr_1.balance,\n",
    "                    x = resampled_data_pr_1.age,\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Target: yes\",\n",
    "                    marker = dict(color = 'rgba(240, 136, 200, 0.8)'),\n",
    "                    text= resampled_data_pr_1.y)\n",
    "# For target outcome 'no'\n",
    "trace2 =go.Scatter(\n",
    "                    y = resampled_data_pr_2.balance,\n",
    "                    x = resampled_data_pr_2.age,\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Target: no\",\n",
    "                    marker = dict(color = 'rgba(0, 130, 200, 0.8)'),\n",
    "                    text= resampled_data_pr_2.y)\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "# Plotting the layout\n",
    "layout = dict(title = 'Balance - Age - target',\n",
    "              xaxis= dict(title= 'Age',\n",
    "                          ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'Balance',\n",
    "                          ticklen= 5,zeroline= False),\n",
    "             autosize=False,\n",
    "             width=700,\n",
    "             height=450,)\n",
    "fig = dict(data = data, layout = layout)\n",
    "    \n",
    "iplot(fig)\n",
    "\n",
    "# Looking at the numerical features and its diverse spread\n",
    "plt.style.context('dark_background')\n",
    "resampled_data.hist(bins=20, figsize=(14,10), color='blue')\n",
    "\n",
    "\n",
    "fig = px.violin(resampled_data, y=\"y\", x=\"balance\", box=True, points=\"all\",hover_data=resampled_data.columns,color=\"y\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#To check balances distribution by months,we use a go.Violin \n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "months = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n",
    "\n",
    "for value in months:\n",
    "    fig.add_trace(go.Violin(x=resampled_data['month'][resampled_data['month'] == value],\n",
    "                            y=resampled_data['balance'][resampled_data['month'] == value],name=value,\n",
    "                            box_visible=True,\n",
    "                            meanline_visible=True))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "graphs = pd.concat([resampled_data['month'][resampled_data.y=='yes'].value_counts(),resampled_data['month'][resampled_data.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs = pd.DataFrame(graphs)\n",
    "graphs.reset_index(inplace=True)\n",
    "headers = ['month','yes','no']\n",
    "graphs.columns = headers\n",
    "\n",
    "# multiple line plots\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot('month', 'yes', data=graphs, marker='o', color='red', linewidth=2)\n",
    "plt.plot('month', 'no', data=graphs, marker='x', color='blue', linewidth=2, linestyle='dashed', label=\"no\")\n",
    "# show legend\n",
    "plt.legend()\n",
    "# show graph\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Selecting all data for Month = Oct since I observed from line that represents all the monthly outcomes, that 'October' had the\n",
    "# highest 'Yes' and least 'No'. The chart below is to explore data specific for month 'Ocotber'\n",
    "\n",
    "# Select all columns for October\n",
    "Oct = resampled_data.loc[resampled_data['month'] == 'oct']\n",
    "Oct.head()\n",
    "\n",
    "#Concatenating a new dataframe 'graph' for all counts of target by days in the variable 'Oct' and give the dataframe a right header\n",
    "graphs = pd.concat([Oct['day'][Oct.y=='yes'].value_counts(),Oct['day'][Oct.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs = pd.DataFrame(graphs)\n",
    "graphs.reset_index(inplace=True)\n",
    "headers = ['days','yes','no']\n",
    "graphs.columns = headers\n",
    "\n",
    "# Replace all values 'NaN' with 'Zero' which indicates that for that day no value was recorded for that outcome.\n",
    "graphs.replace(np.nan,0,inplace = True)\n",
    "graphs.head()\n",
    "\n",
    "\n",
    "# multiple line plots. I observed that we tend to have more postive responses towards the end of the month from 21st-31st\n",
    "# Also, due to the pandas version 1.0.5 used for this work, the plot below cannot accept xlabel = 'counts of responses' and \n",
    "# ylabel = 'days within the month' as part parameters for plotting. Hence the lack of labels\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot('days', 'yes', data=graphs, marker='o', color='red', linewidth=2)\n",
    "plt.plot('days', 'no', data=graphs, marker='x', color='blue', linewidth=2, linestyle='dashed', label=\"no\")\n",
    "plt.xticks(np.arange(0, len(graphs['days'])+5, 1))\n",
    "\n",
    "# show legend\n",
    "plt.legend()\n",
    "# show graph\n",
    "plt.show()\n",
    "\n",
    "# To show responses within september also, as it has a similarly response rate with October. Observations within days in Sep\n",
    "# includes a higher positive response pattern and lesser 'no' compared to October\n",
    "\n",
    "# Select all columns for October\n",
    "Sep = resampled_data.loc[resampled_data['month'] == 'sep']\n",
    "Sep.head()\n",
    "\n",
    "#Concatenating a new dataframe 'graph' for all counts of target by days in the variable 'Sep' and give the dataframe a right header\n",
    "graphs = pd.concat([Sep['day'][Sep.y=='yes'].value_counts(),Sep['day'][Sep.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs = pd.DataFrame(graphs)\n",
    "graphs.reset_index(inplace=True)\n",
    "headers = ['days','yes','no']\n",
    "graphs.columns = headers\n",
    "\n",
    "# Replace all values 'NaN' with 'Zero' which indicates that for that day no value was recorded for that outcome.\n",
    "graphs.replace(np.nan,0,inplace = True)\n",
    "graphs.head()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot('days', 'yes', data=graphs, marker='o', color='red', linewidth=2)\n",
    "plt.plot('days', 'no', data=graphs, marker='x', color='blue', linewidth=2, linestyle='dashed', label=\"no\")\n",
    "plt.xticks(np.arange(0, len(graphs['days'])+5, 1))\n",
    "\n",
    "# show legend\n",
    "plt.legend()\n",
    "# show graph\n",
    "plt.show()\n",
    "\n",
    "# May has a very high degree of \"YES\" but also has a high degree of \"NO\"\n",
    "\n",
    "# Select all columns for May\n",
    "May = resampled_data.loc[resampled_data['month'] == 'may']\n",
    "May.head()\n",
    "\n",
    "#Concatenating a new dataframe 'graph' for all counts of target by days in the variable 'May' and give the dataframe a right header\n",
    "graphs = pd.concat([May['day'][May.y=='yes'].value_counts(),May['day'][May.y=='no'].value_counts()],axis=1)\n",
    "headers = ['yes','no']\n",
    "graphs.columns = headers\n",
    "graphs = pd.DataFrame(graphs)\n",
    "graphs.reset_index(inplace=True)\n",
    "headers = ['days','yes','no']\n",
    "graphs.columns = headers\n",
    "\n",
    "# Replace all values 'NaN' with 'Zero' which indicates that for that day no value was recorded for that outcome.\n",
    "graphs.replace(np.nan,0,inplace = True)\n",
    "graphs.head()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot('days', 'yes', data=graphs, marker='o', color='red', linewidth=2)\n",
    "plt.plot('days', 'no', data=graphs, marker='x', color='blue', linewidth=2, linestyle='dashed', label=\"no\")\n",
    "plt.xticks(np.arange(0, len(graphs['days'])+5, 1))\n",
    "\n",
    "# show legend\n",
    "plt.legend()\n",
    "# show graph\n",
    "plt.show()\n",
    "\n",
    "# Output from below shows customers with houses loans tends to have more money within the bank than those without houses\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "resampled_data['balance'][resampled_data.housing=='no'].plot(kind='hist',color='blue')\n",
    "resampled_data['balance'][resampled_data.housing=='yes'].plot(kind='hist',color='skyblue')\n",
    "plt.legend(['yes','no'])\n",
    "plt.xlabel('Balance',size =14)\n",
    "\n",
    "\n",
    "# I noticed more duration during marketing was spent on people with housing loans\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "resampled_data['duration'][resampled_data.housing=='no'].plot(kind='hist',color='blue')\n",
    "resampled_data['duration'][resampled_data.housing=='yes'].plot(kind='hist',color='skyblue')\n",
    "plt.legend(['yes','no'])\n",
    "plt.xlabel('Duration',size =14)\n",
    "\n",
    "\n",
    "# To visualize age distribution by education for those who responded as Yes to the campaign\n",
    "\n",
    "\n",
    "df = resampled_data.loc[resampled_data[\"y\"] == \"yes\"]\n",
    "\n",
    "occupations = resampled_data[\"education\"].unique().tolist()\n",
    "\n",
    "# Get the ages by education\n",
    "primary = df[\"age\"].loc[df[\"education\"] == \"primary\"].values\n",
    "secondary = df[\"age\"].loc[df[\"education\"] == \"secondary\"].values\n",
    "tertiary = df[\"age\"].loc[df[\"education\"] == \"tertiary\"].values\n",
    "unknown = df[\"age\"].loc[df[\"education\"] == \"unknown\"].values\n",
    "\n",
    "ages = [primary,secondary,tertiary,unknown]\n",
    "\n",
    "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)',\n",
    "          'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', \n",
    "          'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)',\n",
    "         'rgba(229, 126, 56, 0.5)', 'rgba(229, 56, 56, 0.5)',\n",
    "         'rgba(174, 229, 56, 0.5)', 'rgba(229, 56, 56, 0.5)']\n",
    "\n",
    "newdata_pack = []\n",
    "\n",
    "for xd, yd, cls in zip(occupations, ages, colors):\n",
    "        newdata_pack.append(go.Box(y=yd,name=xd,boxpoints='all',jitter=0.5,whiskerwidth=0.2,fillcolor=cls,marker=dict(size=2,),\n",
    "            line=dict(width=1),))\n",
    "\n",
    "layout = go.Layout(title='Distribution of Ages by Education',yaxis=dict(autorange=True,showgrid=True,zeroline=True,dtick=5,gridcolor='rgb(255, 255, 255)',gridwidth=1,zerolinecolor='rgb(255, 255, 255)',zerolinewidth=2,),\n",
    "    margin=dict(l=40,r=30,b=80,t=100,),paper_bgcolor='rgb(224,255,246)',plot_bgcolor='rgb(251,251,251)',showlegend=True)\n",
    "\n",
    "fig = go.Figure(data=newdata_pack, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "# To visualize age distribution of marital status for those who responded as YES to the campaign\n",
    "\n",
    "\n",
    "df = resampled_data.loc[resampled_data[\"y\"] == \"yes\"]\n",
    "\n",
    "maritals = resampled_data[\"marital\"].unique().tolist()\n",
    "\n",
    "# Get the ages by education\n",
    "single = df[\"age\"].loc[df[\"marital\"] == \"single\"].values\n",
    "married = df[\"age\"].loc[df[\"marital\"] == \"married\"].values\n",
    "divorced = df[\"age\"].loc[df[\"marital\"] == \"divorced\"].values\n",
    "\n",
    "\n",
    "ages = [single,married,divorced]\n",
    "\n",
    "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)',\n",
    "          'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', \n",
    "          'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)',\n",
    "         'rgba(229, 126, 56, 0.5)', 'rgba(229, 56, 56, 0.5)',\n",
    "         'rgba(174, 229, 56, 0.5)', 'rgba(229, 56, 56, 0.5)']\n",
    "\n",
    "newdata_pack = []\n",
    "\n",
    "for xd, yd, cls in zip(maritals, ages, colors):\n",
    "        newdata_pack.append(go.Box(y=yd,name=xd,boxpoints='all',jitter=0.5,whiskerwidth=0.2,fillcolor=cls,marker=dict(size=2,),\n",
    "            line=dict(width=1),))\n",
    "\n",
    "layout = go.Layout(title='Distribution of Ages by Marital Status',yaxis=dict(autorange=True,showgrid=True,zeroline=True,dtick=5,zerolinecolor='rgb(255, 255, 255)',zerolinewidth=2,\n",
    "    ),margin=dict(l=40,r=30,b=80,t=100,),paper_bgcolor='rgb(224,255,246)',plot_bgcolor='rgb(251,251,251)',showlegend=True)\n",
    "\n",
    "fig = go.Figure(data=newdata_pack, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "# To visualize age distribution by education of those who responded as YES to the campaign\n",
    "\n",
    "\n",
    "df = resampled_data.loc[resampled_data[\"y\"] == \"yes\"]\n",
    "\n",
    "job_type = resampled_data[\"job\"].unique().tolist()\n",
    "\n",
    "# Get the ages by job type\n",
    "management = df[\"age\"].loc[df[\"job\"] == \"management\"].values\n",
    "admin = df[\"age\"].loc[df[\"job\"] == \"admin.\"].values\n",
    "technician = df[\"age\"].loc[df[\"job\"] == \"technician\"].values\n",
    "services = df[\"age\"].loc[df[\"job\"] == \"services\"].values\n",
    "retired = df[\"age\"].loc[df[\"job\"] == \"retired\"].values\n",
    "blue_collar = df[\"age\"].loc[df[\"job\"] == \"blue-collar\"].values\n",
    "unemployed = df[\"age\"].loc[df[\"job\"] == \"unemployed\"].values\n",
    "entrepreneur = df[\"age\"].loc[df[\"job\"] == \"entrepreneur\"].values\n",
    "housemaid = df[\"age\"].loc[df[\"job\"] == \"housemaid\"].values\n",
    "self_employed = df[\"age\"].loc[df[\"job\"] == \"self-employed\"].values\n",
    "student = df[\"age\"].loc[df[\"job\"] == \"student\"].values\n",
    "unknown = df[\"age\"].loc[df[\"job\"] == \"unknown\"].values\n",
    "\n",
    "\n",
    "ages = [management,blue_collar,technician,admin,services,retired,self_employed,student,entrepreneur,unemployed,housemaid,unknown]\n",
    "\n",
    "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)',\n",
    "          'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', \n",
    "          'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)',\n",
    "         'rgba(229, 126, 56, 0.5)', 'rgba(229, 56, 56, 0.5)',\n",
    "         'rgba(174, 229, 56, 0.5)', 'rgba(229, 56, 56, 0.5)']\n",
    "\n",
    "newdata_pack = []\n",
    "\n",
    "for xd, yd, cls in zip(job_type, ages, colors):\n",
    "        newdata_pack.append(go.Box(y=yd,name=xd,boxpoints='all',jitter=0.5,whiskerwidth=0.2,fillcolor=cls,marker=dict(size=2,),\n",
    "            line=dict(width=1),))\n",
    "\n",
    "layout = go.Layout(title='Distribution of Ages by Job type',yaxis=dict(autorange=True,showgrid=True,zeroline=True,dtick=5,gridcolor='rgb(255, 255, 255)',gridwidth=1,zerolinecolor='rgb(255, 255, 255)',zerolinewidth=2,),\n",
    "    margin=dict(l=40,r=30,b=80,t=100,),\n",
    "    paper_bgcolor='rgb(224,255,246)',\n",
    "    plot_bgcolor='rgb(251,251,251)',\n",
    "    showlegend=True)\n",
    "\n",
    "fig = go.Figure(data=newdata_pack, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# SECTION 5 MODELLING, FEATURE SELECTION PROCESSING AND CROSS VALIDATION\n",
    "\n",
    "# Creating label encoders to treat all categorical variables, the reason for selecting label encoder is because they features \n",
    "# in most columns are either ordinal or just binary which prevents the possiblity of model wrongly capturing their \n",
    "# relationship in an order\n",
    "labelencoder = LabelEncoder()\n",
    "resampled_data[\"job\"] = labelencoder.fit_transform(resampled_data[\"job\"])\n",
    "resampled_data[\"marital\"] = labelencoder.fit_transform(resampled_data[\"marital\"])\n",
    "resampled_data[\"education\"] = labelencoder.fit_transform(resampled_data[\"education\"])\n",
    "resampled_data[\"default\"] = labelencoder.fit_transform(resampled_data[\"default\"])\n",
    "resampled_data[\"housing\"] = labelencoder.fit_transform(resampled_data[\"housing\"])\n",
    "resampled_data[\"loan\"] = labelencoder.fit_transform(resampled_data[\"loan\"])\n",
    "resampled_data[\"contact\"] = labelencoder.fit_transform(resampled_data[\"contact\"])\n",
    "resampled_data[\"month\"] = labelencoder.fit_transform(resampled_data[\"month\"])\n",
    "resampled_data[\"poutcome\"] = labelencoder.fit_transform(resampled_data[\"poutcome\"])\n",
    "\n",
    "resampled_data = banking_data = pd.read_csv(\"C:\\\\Users\\\\dadai\\\\Documents\\\\python_bootcamp\\\\resampled_data.csv\")\n",
    "#resampled_data.to_csv('resampled_data.csv')\n",
    "\n",
    "working_data = resampled_data.copy()\n",
    "\n",
    "working_data.drop(['pdays'], axis=1,inplace = True)\n",
    "working_data.drop(['y'], axis=1,inplace = True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_data = pd.DataFrame(StandardScaler().fit_transform(working_data),columns=working_data.columns)\n",
    "\n",
    "jsonstr = resampled_data.to_json(orient='records')\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
    "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
    "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
    "        <script>\n",
    "          var data = {jsonstr};\n",
    "          document.querySelector(\"#elem\").data = data;\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(jsonstr=jsonstr)\n",
    "display(HTML(html))\n",
    "\n",
    "scaled_data.head()\n",
    "\n",
    "# Feature Extraction with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 15)\n",
    "pca_data = pca.fit(scaled_data)\n",
    "PCA_Component = pd.DataFrame(pca_data.components_, columns=list(scaled_data.columns))\n",
    "\n",
    "# Calculate the variance, however multiply by 100 to make it a percentage, and next step is to label each principal component output in the numpy array 'percentage_variance'\n",
    "percentage_variance =np.round(pca.explained_variance_ratio_*100, decimals =1)\n",
    "labels = ['PC'+ str(x) for x in range(1,len(percentage_variance)+1)]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(x=range(1,len(percentage_variance)+1), height = percentage_variance, tick_label = labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "# The scree plot below is the chart above in a line plot to enable me see the elbow curve clearly which indicates the best size of principal components that explained a proportion of variance\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.show()\n",
    "\n",
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14], [\"First component\", \"Second component\",\"Third component\",\"Fourth component\",\"Fifth component\",\"Sixth component\",\"Seventh component\",\"Eighth component\",\n",
    "                                                 \"Ninth component\",\"Tenth component\",\"Eleventh component\",\"Twelveth component\",\"Thirteenth component\",\"Fourteenth component\",\"Fifteenth component\"])\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(scaled_data.columns)),scaled_data.columns, rotation=90, ha='left')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\")\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % pca_data.explained_variance_ratio_)\n",
    "print(\" \")\n",
    "\n",
    "# The threshold I decided to set is the point/features where the cumulative proportion of the variance explained surpasses 70%\n",
    "print (\"Cumulative Prop. Variance Explained: \", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "print (\" \")\n",
    "print(pca_data.components_)\n",
    "\n",
    "\n",
    "## DECISION TREE CLASSIFER\n",
    "\n",
    "GridSearch Cross Validation\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# This Gridsearch cv process ran for approximately 2 minutes 45 seconds, however this can be reduced by using RandomizedSearcH CV\n",
    "\n",
    "# Add a new column 'class' housing the newly relabelled target column for this process\n",
    "resampled_data['class'] = resampled_data['y'].map({'no':0,'yes':1})\n",
    "\n",
    "# Select all features excluding column 'y'\n",
    "x = working_data.values[:,0:15]\n",
    "target = resampled_data['class']\n",
    "\n",
    "# The following hyperparameters would be iterated over to select the best possible combination\n",
    "parameters = {'criterion':(\"entropy\",\"gini\"),'splitter':(\"random\",\"best\"),'max_depth':range(3,20),\n",
    "              'min_samples_leaf':range(14,20),'random_state': range(90,100)}\n",
    "\n",
    "# The classifier is created, in this case our model is a decision tree\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(), parameters, n_jobs=4,cv=5)\n",
    "clf.fit(X=x, y=target)\n",
    "\n",
    "# This gives the best possible estimator which further broken down in consequent steps\n",
    "tree_model = clf.best_estimator_\n",
    "print (clf.best_score_, clf.best_params_)\n",
    "\n",
    "# sklearn documentations allows result to be printed out in a dataframe to enable readability\n",
    "gridsearch_results = pd.DataFrame(clf.cv_results_)\n",
    "gridsearch_results.head(5)\n",
    "\n",
    "# Select the most important details which are criterion,splitter,max_depth,\n",
    "# min_samples_leaf,random_state and mean_test_score\n",
    "tuned_par = gridsearch_results[['param_criterion','param_splitter','param_max_depth',\n",
    "                                'param_min_samples_leaf','param_random_state','mean_test_score']]\n",
    "\n",
    "# This is to sort the results in an ascending order, from observation of results the first 6 results only\n",
    "# vary slightly based on a decrease of min_sample_leaf from a range of (1-2) and all 'entropy'\n",
    "tuned_par.sort_values( by=\"mean_test_score\",ascending=False).head(15)\n",
    "\n",
    "# Separating the target variable\n",
    "independent = working_data.values[:,0:15]\n",
    "target = resampled_data.values[:, 16]\n",
    "\n",
    "# Splitting the dataset into train and test (testing hold out)\n",
    "independent_train, independent_test, target_train, target_test = train_test_split(\n",
    "independent, target, test_size = 0.3, random_state = 100) \n",
    "\n",
    "\n",
    "independent,target,independent_train, independent_test, target_train, target_test\n",
    "\n",
    "# Creating the classifier object based on the result of the gridsearch result\n",
    "clf_entropy = DecisionTreeClassifier(criterion =\"entropy\",random_state = 91,max_depth=4,\n",
    "                                  splitter=\"random\",min_samples_leaf=19) \n",
    "\n",
    "# Performing training\n",
    "clf_entropy.fit(independent_train, target_train)\n",
    "\n",
    "# Function to make predictions// clf_object is the same as either clf_gini or clf_entropy\n",
    "target_pred = clf_entropy.predict(independent_test)\n",
    "print(\"Predicted values:\")\n",
    "print(target_pred)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "print(\"Confusion Matrix: \",confusion_matrix(target_test, target_pred))\n",
    "print (\"Accuracy : \",accuracy_score(target_test,target_pred)*100)\n",
    "print(\"Report : \", classification_report(target_test, target_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf_entropy.score(independent_train, target_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(clf_entropy.score(independent_test, target_test)))\n",
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(clf_entropy, out_file=\"tree.dot\", class_names=[\"yes\", \"no\"],\n",
    " feature_names=list(working_data.columns), impurity=False, filled=True)\n",
    "\n",
    "import graphviz\n",
    "with open(\"tree.dot\") as f:\n",
    " dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_decision_tree = roc_auc_score(target_test, clf_entropy.predict_proba(independent_test)[:,1])\n",
    "print(' ')\n",
    "print('ROC_AUC is {} and accuracy rate is {}'.format(auc_decision_tree, clf_entropy.score(independent_test, target_test)))\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "disp = metrics.plot_confusion_matrix(clf_entropy, independent_test, target_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "important = pd.DataFrame(clf_entropy.feature_importances_)\n",
    "with_labels = pd.DataFrame(list(working_data.columns))\n",
    "feature_importance_table = pd.concat([with_labels,important],axis=1)\n",
    "headers = ['Features',\"Feature Importance\"]\n",
    "feature_importance_table.columns = headers\n",
    "feature_importance_table = feature_importance_table.sort_values(by=['Feature Importance'],ascending=False)\n",
    "\n",
    "print(\"Feature importances:\\n{}\".format(feature_importance_table))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RANDOM FOREST MODEL\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# The following hyperparameters would be iterated over to select the best possible combination\n",
    "parameters = {'n_estimators':range(250,350),'max_depth':range(3,20),'max_features':range(3,20),\n",
    "             'bootstrap':(True,False)}\n",
    "\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(), parameters, n_iter=3,cv=5,return_train_score=False)\n",
    "rs.fit(X=x, y=target)\n",
    "\n",
    "# sklearn documentations allows result to be printed out in a dataframe to enable readability\n",
    "rs_results = pd.DataFrame(rs.cv_results_)\n",
    "rs_results.head(5)\n",
    "\n",
    "# Results from the output below:\n",
    "# RandomForestClassifier(n_estimators=272,max_features=7,bootstrap=False,max_depth=14)\n",
    "# RandomForestClassifier(n_estimators=319,max_features=9,bootstrap=False,max_depth=10)\n",
    "# RandomForestClassifier(n_estimators=339,max_features=8,bootstrap=False,max_depth=3)\n",
    "\n",
    "# Select the most important details\n",
    "tuned_par = rs_results[['param_n_estimators','param_max_features','param_max_depth','param_bootstrap','mean_test_score']]\n",
    "\n",
    "# This is to sort the results in an ascending order\n",
    "tuned_par.sort_values( by=\"mean_test_score\",ascending=False).head()\n",
    "\n",
    "# The following outcomes would be iterated each to determine which parameters gives a good accuracy and less overfitting\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Classifier\n",
    "clf1=RandomForestClassifier(n_estimators=272,max_features=7,bootstrap=False,max_depth=14)\n",
    "\n",
    "# This is from the initial split in above cell also used for decision tree\n",
    "independent,target,independent_train, independent_test, target_train, target_test\n",
    "\n",
    "# Fit the model\n",
    "tree = clf1.fit(independent_train,target_train)\n",
    "\n",
    "# Prediction of outcomes\n",
    "target_pred_randomforest =clf1.predict(independent_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(target_test, target_pred_randomforest))\n",
    "\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf1.score(independent_train, target_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(clf1.score(independent_test, target_test)))\n",
    "\n",
    "\n",
    "# For the confusion matrix\n",
    "from sklearn import metrics\n",
    "display = metrics.plot_confusion_matrix(clf1, independent_test, target_test)\n",
    "display.figure_.suptitle(\"Confusion Matrix for Random Forest\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Classifier\n",
    "clf2=RandomForestClassifier(n_estimators=319,max_features=9,bootstrap=False,max_depth=10)\n",
    "\n",
    "tree = clf2.fit(independent_train,target_train)\n",
    "\n",
    "target_pred_randomforest =clf2.predict(independent_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(target_test, target_pred_randomforest))\n",
    "\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf2.score(independent_train, target_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(clf2.score(independent_test, target_test)))\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "display = metrics.plot_confusion_matrix(clf2, independent_test, target_test)\n",
    "display.figure_.suptitle(\"Confusion Matrix for Random Forest\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Classifier\n",
    "clf3=RandomForestClassifier(n_estimators=339,max_features=8,bootstrap=False,max_depth=3)\n",
    "\n",
    "tree = clf3.fit(independent_train,target_train)\n",
    "\n",
    "target_pred_randomforest =clf3.predict(independent_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(target_test, target_pred_randomforest))\n",
    "\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf3.score(independent_train, target_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(clf3.score(independent_test, target_test)))\n",
    "\n",
    "display = metrics.plot_confusion_matrix(clf3, independent_test, target_test)\n",
    "display.figure_.suptitle(\"Confusion Matrix for Random Forest\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "TO CHECK THE FEATURE IMPORTANCES IN THE RANDOM FOREST\n",
    "\n",
    "# To check the features and effect on the tree\n",
    "\n",
    "feature_imp =pd.Series(clf2.feature_importances_,index=['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'previous',\n",
    "       'poutcome']).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "fn=['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'previous',\n",
    "       'poutcome']\n",
    "cn=['yes','no']\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "tree.plot_tree(clf2.estimators_[0],\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True);\n",
    "fig.savefig('rf_individualtree.png')\n",
    "\n",
    "\n",
    "\n",
    "# The forest is really large, so lets try to extract the decision trees that makes up the random forest\n",
    "fn=['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'previous',\n",
    "       'poutcome']\n",
    "cn=['yes','no']\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=900)\n",
    "for index in range(0, 15):\n",
    "    tree.plot_tree(clf2.estimators_[index],\n",
    "                   feature_names = fn, \n",
    "                   class_names=cn,\n",
    "                   filled = True,\n",
    "                   ax = axes[index]);\n",
    "\n",
    "    axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "fig.savefig('rf_treess.png')\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "\n",
    "independent,target,independent_train, independent_test, target_train, target_test\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=10,learning_rate=0.3,n_estimators=100,subsample=0.8,criterion=\"mse\",\n",
    "                                 min_samples_leaf=11,max_depth=3,max_leaf_nodes=10)\n",
    "gbrt.fit(independent_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(independent_train,target_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(independent_test, target_test)))\n",
    "\n",
    "\n",
    "NEURAL NETWORK CLASSIFIER TRIALS\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "mlp.fit(independent_train, target_train)\n",
    "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(independent_train, target_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(independent_test, target_test)))\n",
    "\n",
    "\n",
    "#This neural network classifier are scaled sensitive as such there is a need to scale our dataset before using for prediction\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "Calculated_mean = independent_train.mean(axis=0)\n",
    "Calculated_Standard_deviation = independent_train.std(axis=0)\n",
    "independent_train_scaled = (independent_train - Calculated_mean) / Calculated_Standard_deviation\n",
    "\n",
    "# use THE SAME transformation (using training mean and std) on the test set\n",
    "independent_test_scaled = (independent_test - Calculated_mean) / Calculated_Standard_deviation\n",
    "\n",
    "# Building my neural network with the following steps:\n",
    "neural_clf = MLPClassifier(random_state=42,max_iter=1000)\n",
    "neural_clf.fit(independent_train_scaled, target_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(neural_clf.score(independent_train_scaled, target_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(neural_clf.score(independent_test_scaled, target_test)))\n",
    "\n",
    "GRAPHICAL COMPARISON BETWEEN ROC_AUC AND ACCURACY OF THE THREE MODELS\n",
    "\n",
    "# Calculating the ROC_AUC of the three models\n",
    "auc_dt= roc_auc_score(target_test, clf_gini.predict_proba(independent_test)[:,1])\n",
    "auc_rf= roc_auc_score(target_test, clf.predict_proba(independent_test)[:,1])\n",
    "auc_gbrt = roc_auc_score(target_test, gbrt.predict_proba(independent_test)[:,1])\n",
    "\n",
    "# Calculating the accuracy of the three models\n",
    "accuracy_dt = clf_gini.score(independent_test, target_test)\n",
    "accuracy_rf = clf.score(independent_test, target_test)\n",
    "accuracy_gbrt = gbrt.score(independent_test, target_test)\n",
    "\n",
    "# Create a dataframe to house your above results and change index to column ('Models') to enable you plot a barchart representation\n",
    "data = {'Models': ['Decision Tree','Random Forest','Gradient Boosting'],\n",
    "        'ROC_AUC': [auc_dt,auc_rf,auc_gbrt],\n",
    "        'Accuracy':[accuracy_dt,accuracy_rf,accuracy_gbrt]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame(data,columns=['Models','ROC_AUC','Accuracy'])\n",
    "df.set_index('Models',inplace = True)\n",
    "\n",
    "# Plot a bar chart showing the ROC_AUC and Accuracy of the three models\n",
    "df.plot(kind='bar',color=['red','blue'],title= 'Comparing accuracy and ROC_AUC for each model',ylabel= 'ACCURACY and ROC_AUC',xlabel= 'Models used',figsize=(12,5))\n",
    "plt.xticks(rotation=360)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "target_test = pd.DataFrame(target_test)\n",
    "target_test[0].map({'no':0, 'yes':1})\n",
    "\n",
    "# calculate roc curve\n",
    "dt_fpr, dt_tpr, dt_thresholds = roc_curve(target_test, clf_gini.predict_proba(independent_test)[:,1])\n",
    "rf_fpr, rf_tpr, rf_thresholds= roc_curve(target_test, clf.predict_proba(independent_test)[:,1])\n",
    "gbrt_fpr, gbrt_tpr, gbrt_thresholds = roc_curve(target_test, gbrt.predict_proba(independent_test)[:,1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
